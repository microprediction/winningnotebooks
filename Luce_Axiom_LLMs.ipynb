{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOGjT991bPU+N2SjoxCoUkm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/microprediction/winningnotebooks/blob/main/Luce_Axiom_LLMs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "C_4E5p8fd80E",
        "outputId": "eca3fb43-020b-4aa8-99fd-316989fab87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: winning in /usr/local/lib/python3.10/dist-packages (1.0.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from winning) (1.26.4)\n",
            "Requirement already satisfied: pytest in /usr/local/lib/python3.10/dist-packages (from winning) (7.4.4)\n",
            "Requirement already satisfied: pathlib in /usr/local/lib/python3.10/dist-packages (from winning) (1.0.1)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python3.10/dist-packages (from winning) (0.44.0)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.10/dist-packages (from pytest->winning) (2.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pytest->winning) (24.1)\n",
            "Requirement already satisfied: pluggy<2.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from pytest->winning) (1.5.0)\n",
            "Requirement already satisfied: exceptiongroup>=1.0.0rc8 in /usr/local/lib/python3.10/dist-packages (from pytest->winning) (1.2.2)\n",
            "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pytest->winning) (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from scipy) (1.26.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install winning\n",
        "!pip install pandas\n",
        "!pip install scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Luce's Axiom (Implicit in Softmax)\n",
        "Since it is a softmax,  we obviously can surgically force the transformer to obey Luce."
      ],
      "metadata": {
        "id": "EGEtnTqfIx0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import logging\n",
        "from transformers import logging as transformers_logging\n",
        "from transformers import pipeline\n",
        "transformers_logging.set_verbosity_error()\n",
        "\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def fill_in_missing_word(sentence, exclude_words=None, top_k=20):\n",
        "    # Tokenize the input sentence\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    # Find the index of the masked token\n",
        "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    # Get the model's predictions (logits)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    if exclude_words:\n",
        "        # Get the IDs of the words to exclude\n",
        "        exclude_ids = tokenizer.convert_tokens_to_ids(exclude_words)\n",
        "        # Set their logits to a very low value\n",
        "        logits[0, mask_token_index, exclude_ids] = -float('inf')\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = torch.softmax(logits[0, mask_token_index], dim=-1).squeeze()\n",
        "\n",
        "    # Get the top_k predictions\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
        "\n",
        "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
        "    top_k_probs = top_k_probs.tolist()\n",
        "\n",
        "    # Store the top predictions in a dictionary\n",
        "    predictions = dict(zip(top_k_tokens, top_k_probs))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "# Use the tokenizer's mask token in the sentence\n",
        "sentence = f\"The space-ship was so {tokenizer.mask_token} it was not possible to see what was behind it.\"\n",
        "\n",
        "# First iteration: get original probabilities\n",
        "original_predictions = fill_in_missing_word(sentence, top_k=20)\n",
        "\n",
        "# Extract the top predicted word and its probability\n",
        "first_word = max(original_predictions, key=original_predictions.get)\n",
        "first_prob = original_predictions[first_word]\n",
        "\n",
        "print(\"Original Top Predictions:\")\n",
        "for word, prob in original_predictions.items():\n",
        "    print(f\"{word}: {prob:.6f}\")\n",
        "\n",
        "print(f\"\\nFirst answer: '{first_word}', Probability: {first_prob:.6f}\\n\")\n",
        "\n",
        "# Second iteration: remove the most probable word\n",
        "new_predictions = fill_in_missing_word(sentence, exclude_words=[first_word], top_k=20)\n",
        "\n",
        "print(\"New Top Predictions After Excluding the First Word:\")\n",
        "for word, prob in new_predictions.items():\n",
        "    print(f\"{word}: {prob:.6f}\")\n",
        "\n",
        "# Compare probabilities according to Luce Choice Axiom\n",
        "print(\"\\nComparing Ratios According to Luce Choice Axiom:\\n\")\n",
        "\n",
        "# Calculate the sum of probabilities excluding the first word\n",
        "sum_original_probs_excl_first = sum(original_predictions[word] for word in original_predictions if word != first_word)\n",
        "sum_new_probs = sum(new_predictions.values())\n",
        "\n",
        "print(f\"Sum of original probabilities (excluding '{first_word}'): {sum_original_probs_excl_first:.6f}\")\n",
        "print(f\"Sum of new probabilities: {sum_new_probs:.6f}\")\n",
        "\n",
        "# Verify that sums are approximately equal (should be close to 1 - first_prob)\n",
        "print(f\"1 - Probability of '{first_word}': {1 - first_prob:.6f}\")\n",
        "\n",
        "# Check ratios for each word in new_predictions\n",
        "print(\"\\nWord\\tOriginal Prob\\tExpected New Prob\\tActual New Prob\\tRatio (Actual/Expected)\")\n",
        "for word in new_predictions:\n",
        "    original_prob = original_predictions.get(word, 0)\n",
        "    expected_new_prob = original_prob / (1 - first_prob)\n",
        "    actual_new_prob = new_predictions[word]\n",
        "    ratio = actual_new_prob / expected_new_prob if expected_new_prob != 0 else 0\n",
        "    print(f\"{word}\\t{original_prob:.6f}\\t{expected_new_prob:.6f}\\t{actual_new_prob:.6f}\\t{ratio:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "frh-_bIhH5KW",
        "outputId": "7333a745-b73d-4261-932e-871eb2030e46"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Top Predictions:\n",
            "large: 0.344735\n",
            "small: 0.227211\n",
            "massive: 0.106483\n",
            "huge: 0.058782\n",
            "big: 0.030036\n",
            "close: 0.020969\n",
            "dark: 0.020564\n",
            "enormous: 0.017063\n",
            "low: 0.010211\n",
            "tiny: 0.010106\n",
            "heavy: 0.009116\n",
            "immense: 0.007942\n",
            "tall: 0.007581\n",
            "vast: 0.007423\n",
            "high: 0.006501\n",
            "narrow: 0.004375\n",
            "powerful: 0.003378\n",
            "quiet: 0.003095\n",
            "thin: 0.003060\n",
            "silent: 0.002666\n",
            "\n",
            "First answer: 'large', Probability: 0.344735\n",
            "\n",
            "New Top Predictions After Excluding the First Word:\n",
            "small: 0.346745\n",
            "massive: 0.162502\n",
            "huge: 0.089707\n",
            "big: 0.045837\n",
            "close: 0.032000\n",
            "dark: 0.031383\n",
            "enormous: 0.026040\n",
            "low: 0.015584\n",
            "tiny: 0.015423\n",
            "heavy: 0.013912\n",
            "immense: 0.012120\n",
            "tall: 0.011569\n",
            "vast: 0.011328\n",
            "high: 0.009920\n",
            "narrow: 0.006677\n",
            "powerful: 0.005156\n",
            "quiet: 0.004724\n",
            "thin: 0.004669\n",
            "silent: 0.004069\n",
            "black: 0.003872\n",
            "\n",
            "Comparing Ratios According to Luce Choice Axiom:\n",
            "\n",
            "Sum of original probabilities (excluding 'large'): 0.556562\n",
            "Sum of new probabilities: 0.853237\n",
            "1 - Probability of 'large': 0.655265\n",
            "\n",
            "Word\tOriginal Prob\tExpected New Prob\tActual New Prob\tRatio (Actual/Expected)\n",
            "small\t0.227211\t0.346747\t0.346745\t0.999994\n",
            "massive\t0.106483\t0.162503\t0.162502\t0.999994\n",
            "huge\t0.058782\t0.089707\t0.089707\t0.999994\n",
            "big\t0.030036\t0.045838\t0.045837\t0.999994\n",
            "close\t0.020969\t0.032000\t0.032000\t0.999994\n",
            "dark\t0.020564\t0.031383\t0.031383\t0.999994\n",
            "enormous\t0.017063\t0.026040\t0.026040\t0.999994\n",
            "low\t0.010211\t0.015584\t0.015584\t0.999994\n",
            "tiny\t0.010106\t0.015423\t0.015423\t0.999994\n",
            "heavy\t0.009116\t0.013912\t0.013912\t0.999994\n",
            "immense\t0.007942\t0.012120\t0.012120\t0.999994\n",
            "tall\t0.007581\t0.011569\t0.011569\t0.999994\n",
            "vast\t0.007423\t0.011328\t0.011328\t0.999994\n",
            "high\t0.006501\t0.009920\t0.009920\t0.999994\n",
            "narrow\t0.004375\t0.006677\t0.006677\t0.999994\n",
            "powerful\t0.003378\t0.005156\t0.005156\t0.999994\n",
            "quiet\t0.003095\t0.004724\t0.004724\t0.999994\n",
            "thin\t0.003060\t0.004669\t0.004669\t0.999994\n",
            "silent\t0.002666\t0.004069\t0.004069\t0.999994\n",
            "black\t0.000000\t0.000000\t0.003872\t0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# \"Real\" Choice\n",
        "But what about making actual choices in context? Here we try to ask very similar questions to remove horses from a race."
      ],
      "metadata": {
        "id": "jLaevk3yI8DW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForMaskedLM\n",
        "import pandas as pd\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def fill_in_missing_word(sentence, exclude_words=None, top_k=20):\n",
        "    # Tokenize the input sentence\n",
        "    inputs = tokenizer(sentence, return_tensors='pt')\n",
        "    input_ids = inputs['input_ids']\n",
        "\n",
        "    # Find the index of the masked token\n",
        "    mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "\n",
        "    # Get the model's predictions (logits)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "    if exclude_words:\n",
        "        # Get the IDs of the words to exclude\n",
        "        exclude_ids = tokenizer.convert_tokens_to_ids(exclude_words)\n",
        "        # Set their logits to a very low value\n",
        "        logits[0, mask_token_index, exclude_ids] = -float('inf')\n",
        "\n",
        "    # Apply softmax to get probabilities\n",
        "    probs = torch.softmax(logits[0, mask_token_index], dim=-1)\n",
        "\n",
        "    # Get the top_k predictions\n",
        "    top_k_probs, top_k_indices = torch.topk(probs, top_k, dim=-1)\n",
        "\n",
        "    # If there's only one mask token, adjust dimensions\n",
        "    if top_k_indices.dim() == 2 and top_k_indices.size(0) == 1:\n",
        "        top_k_indices = top_k_indices.squeeze(0)\n",
        "        top_k_probs = top_k_probs.squeeze(0)\n",
        "\n",
        "    top_k_tokens = tokenizer.convert_ids_to_tokens(top_k_indices.tolist())\n",
        "    top_k_probs = top_k_probs.tolist()\n",
        "\n",
        "    # Store the top predictions in a dictionary\n",
        "    predictions = dict(zip(top_k_tokens, top_k_probs))\n",
        "\n",
        "    return predictions\n",
        "\n",
        "\n",
        "# Luce Axiom Check\n",
        "def luce_check(sentence1, sentence2):\n",
        "    # Get probabilities from both sentences\n",
        "    probs1 = fill_in_missing_word(sentence1, top_k=100)\n",
        "    probs2 = fill_in_missing_word(sentence2, top_k=10)\n",
        "\n",
        "    # Filter out words in sentence2 not present in sentence1\n",
        "    common_tokens = set(probs1.keys()).intersection(set(probs2.keys()))\n",
        "\n",
        "    # Create filtered dictionaries with common tokens\n",
        "    probs1_filtered = {token: probs1[token] for token in common_tokens}\n",
        "    probs2_filtered = {token: probs2[token] for token in common_tokens}\n",
        "\n",
        "    # Store the original scores (unnormalized) before renormalization\n",
        "    original_scores = {token: probs1[token] for token in probs1_filtered}\n",
        "\n",
        "\n",
        "    # Renormalize probs2 so they sum to 1\n",
        "    total_prob2 = sum(probs2_filtered.values())\n",
        "    probs2_normalized = {token: prob / total_prob2 for token, prob in probs2_filtered.items()}\n",
        "\n",
        "    # Renormalize probs1 so they sum to 1\n",
        "    total_prob1 = sum(probs1_filtered.values())\n",
        "    probs1_normalized = {token: prob / total_prob1 for token, prob in probs1_filtered.items()}\n",
        "\n",
        "\n",
        "    # Merge both into a DataFrame for comparison\n",
        "    df = pd.DataFrame({\n",
        "        'Choice': list(probs1_normalized.keys()),\n",
        "        'Original score': [original_scores[token] for token in probs1_normalized.keys()],\n",
        "        'Implied score': list(probs1_normalized.values()),\n",
        "        'Actual score': [probs2_normalized[token] for token in probs1_normalized.keys()]\n",
        "    })\n",
        "\n",
        "    # Add a column for the empirical / Luce ratio\n",
        "    df['Ratio'] = df['Actual score'] / df['Implied score']\n",
        "    df.sort_values('Implied score',inplace=True, ascending=False)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Function to display results\n",
        "def display_predictions(sentence, exclude_words=None):\n",
        "    predictions = fill_in_missing_word(sentence, exclude_words=exclude_words)\n",
        "    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
        "    print(f\"Sentence: {sentence}\")\n",
        "    print(\"Top predictions:\")\n",
        "    for word, score in sorted_predictions:\n",
        "        print(f\"  {word}: {score:.4f}\")\n",
        "    print(\"\\n\")\n",
        "    return sorted_predictions\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bIacGX3KI-IG",
        "outputId": "4f913fa0-f073-4175-c7e1-0dea4d0342bc"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Eating"
      ],
      "metadata": {
        "id": "GVrd7WWOSHLx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favourite type of fruit or vegetable is called a {tokenizer.mask_token} and I eat one of them every day.\"\n",
        "sentence2 = f\"My favourite type of fruit is called a {tokenizer.mask_token} and I eat one of them every day.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JYIN_kaSInU",
        "outputId": "386964e8-200a-4bf7-ba3c-2e4ba3778c50"
      },
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Choice  Original score  Implied score  Actual score     Ratio\n",
            "9      banana        0.114603       0.266236      0.178440  0.670235\n",
            "4       mango        0.059991       0.139366      0.249007  1.786718\n",
            "2         nut        0.052449       0.121846      0.068018  0.558233\n",
            "1       fruit        0.034807       0.080861      0.088250  1.091377\n",
            "0       peach        0.032701       0.075967      0.077764  1.023651\n",
            "6       berry        0.032125       0.074630      0.053912  0.722390\n",
            "3        plum        0.029800       0.069228      0.067843  0.980001\n",
            "7       lemon        0.027329       0.063488      0.058413  0.920065\n",
            "5  strawberry        0.024800       0.057614      0.056728  0.984631\n",
            "8      cherry        0.021852       0.050765      0.101623  2.001842\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# States"
      ],
      "metadata": {
        "id": "enrzyInsNktx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favourite state in the U.S. is {tokenizer.mask_token} and I try to visit once a year.\"\n",
        "sentence2 = f\"My favourite Western state in the U.S. is {tokenizer.mask_token} and I try to visit once a year.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVMy20U_NXTo",
        "outputId": "322d8fe0-161f-4ef6-d7d3-646d3625a7db"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Choice  Original score  Implied score  Actual score     Ratio\n",
            "6  california        0.108853       0.298702      0.201740  0.675390\n",
            "9     arizona        0.067399       0.184951      0.100027  0.540833\n",
            "1       texas        0.055019       0.150977      0.076750  0.508360\n",
            "8    colorado        0.031841       0.087376      0.078146  0.894368\n",
            "7      oregon        0.028873       0.079231      0.085660  1.081137\n",
            "5    oklahoma        0.019698       0.054052      0.066355  1.227612\n",
            "3      nevada        0.016581       0.045499      0.101803  2.237464\n",
            "4     montana        0.015908       0.043653      0.142662  3.268112\n",
            "2       idaho        0.010460       0.028704      0.049947  1.740053\n",
            "0     wyoming        0.009787       0.026856      0.096909  3.608484\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Countries"
      ],
      "metadata": {
        "id": "aNtCFUAWNra6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favourite drink is {tokenizer.mask_token} and it tastes great.\"\n",
        "sentence2 = f\"My favourite alcoholic drink is {tokenizer.mask_token} and it tastes great.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P3ZDa9S1NttO",
        "outputId": "74e40f04-b67d-4417-f6c0-900e442216a3"
      },
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Choice  Original score  Implied score  Actual score     Ratio\n",
            "6      water        0.087032       0.196648      0.148692  0.756133\n",
            "8     coffee        0.079112       0.178752      0.075745  0.423745\n",
            "7        gin        0.061759       0.139543      0.169855  1.217224\n",
            "1       wine        0.048430       0.109426      0.080789  0.738293\n",
            "5        rum        0.034824       0.078685      0.134153  1.704928\n",
            "2       beer        0.034671       0.078338      0.072892  0.930489\n",
            "4  champagne        0.032639       0.073749      0.045811  0.621178\n",
            "9      vodka        0.028924       0.065353      0.144282  2.207720\n",
            "0       cola        0.019985       0.045155      0.051135  1.132432\n",
            "3    bourbon        0.015203       0.034351      0.076646  2.231272\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pets"
      ],
      "metadata": {
        "id": "HvdEfNEeV8Bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favorite type of pet or farm animal is a {tokenizer.mask_token}, and it is very loyal.\"\n",
        "sentence2 = f\"My favorite type of pet is a {tokenizer.mask_token}, and it is very loyal.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2GlcJd1Wu_g",
        "outputId": "d5fff309-833e-4eee-9e90-03dd2e9f1647"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Choice  Original score  Implied score  Actual score     Ratio\n",
            "9        dog        0.111114       0.227469      0.170574  0.749878\n",
            "2        cat        0.071628       0.146635      0.270100  1.841989\n",
            "5       wolf        0.061467       0.125834      0.082584  0.656294\n",
            "6       bear        0.057627       0.117972      0.115044  0.975183\n",
            "8       lion        0.044970       0.092062      0.127784  1.388024\n",
            "3      tiger        0.040343       0.082590      0.041469  0.502112\n",
            "0     rabbit        0.039527       0.080919      0.051705  0.638977\n",
            "7  chihuahua        0.033485       0.068549      0.033833  0.493551\n",
            "4    leopard        0.017254       0.035322      0.068246  1.932106\n",
            "1      mouse        0.011063       0.022648      0.038661  1.706991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Furniture"
      ],
      "metadata": {
        "id": "KoGPihHFXNx8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"I need to buy a new piece of furniture or appliance called a {tokenizer.mask_token} for my house.\"\n",
        "sentence2 = f\"I need to buy a new piece of furniture called a {tokenizer.mask_token} for my house.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bASNdgtsWxDn",
        "outputId": "3b27156e-00e2-47eb-962a-fb92f70af576"
      },
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Choice  Original score  Implied score  Actual score     Ratio\n",
            "2      piano        0.012473       0.219646      0.098736  0.449523\n",
            "1       room        0.007054       0.124207      0.127745  1.028484\n",
            "5  furniture        0.006571       0.115705      0.121403  1.049243\n",
            "9      chair        0.005992       0.105506      0.114693  1.087075\n",
            "4    bedroom        0.005213       0.091797      0.085581  0.932283\n",
            "3        bed        0.004793       0.084400      0.106573  1.262715\n",
            "8       sofa        0.004417       0.077779      0.103240  1.327352\n",
            "7      table        0.004067       0.071618      0.088269  1.232504\n",
            "6       desk        0.003359       0.059155      0.071658  1.211344\n",
            "0    dresser        0.002850       0.050187      0.082102  1.635945\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Classics"
      ],
      "metadata": {
        "id": "OzL4VPWnXT_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"They are studying modern or classical {tokenizer.mask_token} at the university.\"\n",
        "sentence2 = f\"They are studying classical {tokenizer.mask_token} at the university.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5zGAiuaW6MO",
        "outputId": "0438babc-f8e3-4531-ccd2-9466d61eb5e7"
      },
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "        Choice  Implied score  Actual score      Ratio\n",
            "8        music       0.573795      0.500643   0.872513\n",
            "0    languages       0.189659      0.100392   0.529329\n",
            "3   literature       0.071643      0.075768   1.057582\n",
            "7      history       0.063840      0.026200   0.410404\n",
            "4        dance       0.053870      0.046529   0.863729\n",
            "5      studies       0.015291      0.031336   2.049256\n",
            "9  composition       0.013594      0.018114   1.332457\n",
            "6        greek       0.007472      0.023592   3.157258\n",
            "1    philology       0.006550      0.150025  22.905527\n",
            "2       ballet       0.004285      0.027400   6.394469\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Big cats"
      ],
      "metadata": {
        "id": "lqHrV35KXWar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favorite animal at the zoo is the {tokenizer.mask_token}, and I always visit its enclosure.\"\n",
        "sentence2 = f\"My favorite big cat at the zoo is the {tokenizer.mask_token}, and I always visit its enclosure.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nR17BZ74XDKm",
        "outputId": "49fe61e6-925b-401f-a857-53f7899f1c99"
      },
      "execution_count": 103,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Choice  Implied score  Actual score     Ratio\n",
            "6     lion       0.546050      0.204367  0.374264\n",
            "3    tiger       0.151210      0.201513  1.332673\n",
            "5     bear       0.072922      0.080488  1.103761\n",
            "4  leopard       0.068827      0.075231  1.093049\n",
            "2  panther       0.063777      0.088980  1.395177\n",
            "1      cat       0.062657      0.294430  4.699074\n",
            "0    mouse       0.034558      0.054991  1.591258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting Around"
      ],
      "metadata": {
        "id": "DXrf5KcCXeWu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My preferred mode of transportation is the {tokenizer.mask_token}, it's very convenient.\"\n",
        "sentence2 = f\"My preferred public transportation is the {tokenizer.mask_token}, it's very convenient.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqQZmF_uXgrT",
        "outputId": "693e0057-2479-469d-b732-78651551e7e1"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Choice  Implied score  Actual score     Ratio\n",
            "0      car       0.293049      0.186921  0.637848\n",
            "4      bus       0.221204      0.195267  0.882744\n",
            "6     road       0.101527      0.046570  0.458697\n",
            "5   subway       0.086666      0.210164  2.424987\n",
            "2  airport       0.081799      0.089627  1.095710\n",
            "3  highway       0.078798      0.054571  0.692536\n",
            "7    train       0.058760      0.050261  0.855363\n",
            "1    buses       0.033907      0.073449  2.166226\n",
            "8  freeway       0.031542      0.046425  1.471831\n",
            "9    metro       0.012748      0.046745  3.666779\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Holidays"
      ],
      "metadata": {
        "id": "Yu48jL0-Xs9Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favorite holiday is {tokenizer.mask_token}, and I always celebrate it with family.\"\n",
        "sentence2 = f\"My favorite winter holiday is {tokenizer.mask_token}, and I always celebrate it with family.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df[:3])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EnhNAxUfXvGN",
        "outputId": "f0ff68f4-68dc-46d3-84ef-8257a9bde00e"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "         Choice  Implied score  Actual score     Ratio\n",
            "7     christmas       0.854978      0.545480  0.638004\n",
            "1  thanksgiving       0.092472      0.132594  1.433873\n",
            "8     halloween       0.023282      0.030875  1.326135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# And some more..."
      ],
      "metadata": {
        "id": "e4aLHglSX6Kj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Example 1\n",
        "sentence1 = f\"My favorite city to visit is {tokenizer.mask_token}, and I go there every summer.\"\n",
        "sentence2 = f\"My favorite French city to visit is {tokenizer.mask_token}, and I go there every summer.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Pwk_OsoYWCk",
        "outputId": "35b2b226-8e2d-4449-ca31-503b00fa61f7"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Choice  Implied score  Actual score     Ratio\n",
            "0   paris       0.815324      0.840575  1.030970\n",
            "2    nice       0.106383      0.150050  1.410467\n",
            "1  geneva       0.078292      0.009375  0.119748\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNVK7rQLYaG4",
        "outputId": "41a673a6-58ec-4333-b790-ebdbf5fd5610"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Choice  Implied score  Actual score     Ratio\n",
            "6    chicken       0.685280      0.147544  0.215305\n",
            "4       rice       0.095997      0.071000  0.739607\n",
            "2      china       0.077318      0.136574  1.766379\n",
            "0    chinese       0.062546      0.334283  5.344620\n",
            "5  cantonese       0.028176      0.079458  2.820050\n",
            "3   japanese       0.022814      0.130745  5.730838\n",
            "7       thai       0.021291      0.062383  2.929993\n",
            "1      curry       0.006577      0.038014  5.779574\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"The best car brand, in my opinion, is {tokenizer.mask_token}.\"\n",
        "sentence2 = f\"The best German car brand, in my opinion, is {tokenizer.mask_token}.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z3bimqxYm-j",
        "outputId": "11de4b4f-66cb-4fd7-908b-0fa5b8d17303"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Choice  Implied score  Actual score     Ratio\n",
            "0         bmw       0.333629      0.606744  1.818619\n",
            "4    mercedes       0.207512      0.110781  0.533857\n",
            "3        ford       0.187307      0.026922  0.143733\n",
            "2     ferrari       0.099664      0.033399  0.335118\n",
            "6     porsche       0.082868      0.117889  1.422624\n",
            "7     renault       0.058201      0.042578  0.731572\n",
            "5  volkswagen       0.015427      0.037029  2.400227\n",
            "1        audi       0.015393      0.024657  1.601792\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Programming languages"
      ],
      "metadata": {
        "id": "ipPL-8uTY_RP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence1 = f\"My favorite programming language is {tokenizer.mask_token}, I use it daily.\"\n",
        "sentence2 = f\"My favorite object-oriented programming language is {tokenizer.mask_token}, I use it daily.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylVmW2qgYyxV",
        "outputId": "06e6a482-a163-4dfe-c8de-6c0bda58b372"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Choice  Implied score  Actual score     Ratio\n",
            "6    java       0.334261      0.430050  1.286569\n",
            "7       c       0.294080      0.267487  0.909570\n",
            "1  python       0.140631      0.125992  0.895908\n",
            "0     php       0.092112      0.024155  0.262237\n",
            "5   basic       0.060059      0.017473  0.290924\n",
            "4    html       0.040316      0.006228  0.154483\n",
            "3  pascal       0.021915      0.100941  4.606001\n",
            "2     sql       0.008406      0.013398  1.593861\n",
            "8    ruby       0.008220      0.014276  1.736783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Starting the day?"
      ],
      "metadata": {
        "id": "njHD9X2wY4Gv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"I like to drink {tokenizer.mask_token} in the morning to start my day.\"\n",
        "sentence2 = f\"I like to drink hot {tokenizer.mask_token} in the morning to start my day.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cb7oM4tcY26Z",
        "outputId": "8f2d8349-726d-445a-f767-742ce8929191"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Choice  Implied score  Actual score     Ratio\n",
            "3  coffee       0.549390      0.238140  0.433463\n",
            "2   water       0.227673      0.657931  2.889812\n",
            "1    beer       0.158892      0.008129  0.051160\n",
            "4     tea       0.028318      0.059600  2.104672\n",
            "5    milk       0.027269      0.006568  0.240859\n",
            "0  drinks       0.008458      0.029632  3.503310\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Scientists"
      ],
      "metadata": {
        "id": "k_soG6r5ZIht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favorite scientist is {tokenizer.mask_token}, they changed the world.\"\n",
        "sentence2 = f\"My favorite 19th Century scientist is {tokenizer.mask_token}, they changed the world.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yZo4xOM9ZH1V",
        "outputId": "ed79f3ea-42f9-4835-de55-706490c85712"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Choice  Implied score  Actual score     Ratio\n",
            "3  einstein       0.685723      0.186807  0.272423\n",
            "0      here       0.106623      0.239498  2.246216\n",
            "5       you       0.056903      0.091914  1.615287\n",
            "9      back       0.030993      0.076372  2.464219\n",
            "4      dead       0.028130      0.049693  1.766531\n",
            "1        me       0.027055      0.085118  3.146138\n",
            "8     right       0.022430      0.070561  3.145860\n",
            "7   charles       0.017279      0.078950  4.569118\n",
            "2      gone       0.012904      0.070766  5.483961\n",
            "6        he       0.011961      0.050322  4.207103\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# More pets"
      ],
      "metadata": {
        "id": "OUVTnWekZTI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My preferred pet is a {tokenizer.mask_token}, they make great companions.\"\n",
        "sentence2 = f\"My preferred small pet is a {tokenizer.mask_token}, they make great companions.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9PmEd2SX82L",
        "outputId": "a933de68-a637-4c97-b2de-d8d7b871a070"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Choice  Implied score  Actual score     Ratio\n",
            "2       cat       0.155339      0.160443  1.032855\n",
            "5      bear       0.141039      0.127901  0.906848\n",
            "8      lion       0.130099      0.090659  0.696849\n",
            "0      deer       0.127077      0.132946  1.046185\n",
            "6      wolf       0.115865      0.095933  0.827969\n",
            "3     tiger       0.103580      0.087945  0.849057\n",
            "9       dog       0.089925      0.094879  1.055091\n",
            "4  squirrel       0.060674      0.082661  1.362362\n",
            "1     mouse       0.038420      0.068674  1.787468\n",
            "7    parrot       0.037983      0.057961  1.525957\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favorite baby name is {tokenizer.mask_token}, and I always associate it with family.\"\n",
        "sentence2 = f\"My favorite girl's baby name is {tokenizer.mask_token}, and I always associate it with family.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "RcNJv4kGuZMh",
        "outputId": "d86f5410-1e7d-4c7f-dc7d-6c0cacd61b65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     Choice  Implied score  Actual score     Ratio\n",
            "3      lily       0.203468      0.145201  0.713629\n",
            "5     emily       0.150642      0.162862  1.081119\n",
            "0       mia       0.112507      0.131783  1.171325\n",
            "4     bella       0.111161      0.088390  0.795158\n",
            "8     sarah       0.088915      0.094142  1.058783\n",
            "7    rachel       0.076603      0.071505  0.933443\n",
            "9      kate       0.076348      0.084366  1.105008\n",
            "1    lauren       0.061055      0.072298  1.184147\n",
            "6  brittany       0.060399      0.075525  1.250435\n",
            "2     chloe       0.058902      0.073929  1.255132\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favorite baseball team are the {tokenizer.mask_token}, and I always associate them with success.\"\n",
        "sentence2 = f\"My favorite new york baseball team are the {tokenizer.mask_token}, and I always associate them with success.\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df[:3])"
      ],
      "metadata": {
        "id": "n-0Bg3-OuwXf",
        "outputId": "968f7af8-78ef-4862-f6cb-f9f949708df1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 135,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Choice  Implied score  Actual score     Ratio\n",
            "5  yankees       0.264528      0.401384  1.517361\n",
            "3   tigers       0.186640      0.017931  0.096070\n",
            "2     mets       0.179004      0.433763  2.423203\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence1 = f\"My favorite age is {tokenizer.mask_token}, and my daughter is that age\"\n",
        "sentence2 = f\"My favorite age for middle school kids is {tokenizer.mask_token}, and my daughter is that age\"\n",
        "df = luce_check(sentence1, sentence2)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "p_JRORWrvH1C",
        "outputId": "f9f94bf0-1c21-4970-a357-56da90d9aaae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "   Choice  Implied score  Actual score     Ratio\n",
            "7   eight       0.155288      0.152468  0.981840\n",
            "9  twelve       0.138472      0.139279  1.005830\n",
            "4  eleven       0.128694      0.085072  0.661040\n",
            "0   seven       0.113845      0.085769  0.753386\n",
            "5    five       0.113015      0.084129  0.744404\n",
            "3      16       0.094257      0.103968  1.103035\n",
            "2     six       0.076396      0.089710  1.174276\n",
            "1      12       0.066326      0.116064  1.749902\n",
            "6       9       0.063752      0.068647  1.076778\n",
            "8      13       0.049956      0.074894  1.499213\n"
          ]
        }
      ]
    }
  ]
}